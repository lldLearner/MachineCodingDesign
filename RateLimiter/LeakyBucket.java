class LeakyBucket {
    private final int capacity;         // max tokens that bucket can hold
    private final double leakRate;      // tokens leaked per second
    private double tokens = 0;          // current tokens
    private long lastLeakTime;

    public LeakyBucket(int capacity, double leakRate) {
        this.capacity = capacity;
        this.leakRate = leakRate;
        this.lastLeakTime = System.nanoTime();
    }

    private void leak() {
        long now = System.nanoTime();
        double elapsedSeconds = (now - lastLeakTime) / 1_000_000_000.0;
        double leaked = elapsedSeconds * leakRate;
        tokens = Math.max(0, tokens - leaked);
        lastLeakTime = now;
    }

    public synchronized boolean allowRequest() {
        leak();
        if (tokens < capacity) {
            tokens += 1;
            return true;
        }
        return false;
    }

    public static void main(String[] args) throws InterruptedException {
      LeakyBucket bucket = new LeakyBucket(3, 1); // capacity=3, leak=1 token/sec
  
      System.out.println(bucket.allowRequest()); // true
      System.out.println(bucket.allowRequest()); // true
      System.out.println(bucket.allowRequest()); // true
      System.out.println(bucket.allowRequest()); // false (limit hit)
  
      Thread.sleep(1100); // wait 1.1 sec â†’ 1 token leaked out
      System.out.println(bucket.allowRequest()); // true
  }
}

/////Per User Locking

import java.util.concurrent.ConcurrentHashMap;

class UserRateLimiter {
    private final ConcurrentHashMap<String, LeakyBucket> buckets = new ConcurrentHashMap<>();
    private final int capacity;
    private final double leakRate;

    public UserRateLimiter(int capacity, double leakRate) {
        this.capacity = capacity;
        this.leakRate = leakRate;
    }

    public boolean allowRequest(String userId) {
        // ensures only one bucket per user (thread-safe)
        buckets.putIfAbsent(userId, new LeakyBucket(capacity, leakRate));

        LeakyBucket bucket = buckets.get(userId);
        return bucket.allowRequest();
    }

    public static void main(String[] args) throws InterruptedException {
      UserRateLimiter limiter = new UserRateLimiter(3, 1); // 3 requests, leak 1/sec
  
      System.out.println(limiter.allowRequest("u1")); // true
      System.out.println(limiter.allowRequest("u1")); // true
      System.out.println(limiter.allowRequest("u1")); // true
      System.out.println(limiter.allowRequest("u1")); // false
  
      System.out.println(limiter.allowRequest("u2")); // true (new bucket)
  }

}

/*
  ðŸŸ¦ Now the interviewer switches into CONCURRENCY discussion.

They ALWAYS do this next.

And to get Strong Hire, your answers need to be crisp.

Below is the exact script you give:

ðŸ”¥ CONCURRENCY DEEP DIVE (Strong Hire Answer)
âœ” Why is this thread-safe?
1. ConcurrentHashMap

Safe for concurrent put/get

Lock striping â€” reduces contention

Per-key locking â†’ different users donâ€™t block each other

2. Inside each bucket: synchronized allowRequest()

Each userâ€™s requests synchronize on their own bucket, so:

Only requests for SAME USER block each other

Requests for DIFFERENT users DO NOT block

This is fine-grained locking â†’ low contention

Strong line to say:

â€œLock granularity is per-user, not global. This is optimal because each userâ€™s state is isolated.â€

3. Memory Visibility

synchronized ensures:

â€œHappens-beforeâ€ guarantees

Visibility of updated tokens across threads

No stale reads

Say this:

â€œThe synchronized block ensures visibility and prevents race conditions on token count.â€

âœ” What if interviewer asks:
â€œWhy not use AtomicDouble or ReentrantLock?â€

Answer:

â€œAtomicDouble does not exist and custom CAS loops introduce floating point inconsistency.
ReentrantLock is fine, but synchronized is cheaper for uncontended paths and clearer.â€

âœ” Contention & Scalability

You must say:

â€œContention only happens on requests for the same user, which is acceptable.
Different users never block each other because each bucket has its own lock.â€

THIS is what gets you Strong Hire.

âœ” Thread Safety Problem to Point Out

Even though buckets are created with putIfAbsent, the internal allowRequest() must be synchronized, otherwise:

Two threads may leak incorrectly

Token increments collide

Token count may exceed capacity

Being able to articulate this is critical.
*/

///// Credit System

class CreditBucket {
    private final int baseCapacity;      // base allowed limit per second
    private final int maxCredits;        // additional capacity from unused requests
    private final double refillRate;     // tokens added per second
    private double tokens;               // current tokens
    private long lastRefillTime;

    public CreditBucket(int baseCapacity, int maxCredits, double refillRate) {
        this.baseCapacity = baseCapacity;
        this.maxCredits = maxCredits;
        this.refillRate = refillRate;
        this.tokens = baseCapacity;  // start full
        this.lastRefillTime = System.nanoTime();
    }

    private void refill() {
        long now = System.nanoTime();
        double seconds = (now - lastRefillTime) / 1_000_000_000.0;

        tokens = Math.min(baseCapacity + maxCredits, tokens + seconds * refillRate);
        lastRefillTime = now;
    }

    public synchronized boolean allowRequest() {
        refill();

        if (tokens >= 1) {
            tokens -= 1;
            return true;
        }
        return false;
    }
}


import java.util.concurrent.ConcurrentHashMap;

class UserCreditLimiter {
    private final ConcurrentHashMap<String, CreditBucket> buckets = new ConcurrentHashMap<>();
    private final int baseCapacity;
    private final int maxCredits;
    private final double refillRate;

    public UserCreditLimiter(int baseCapacity, int maxCredits, double refillRate) {
        this.baseCapacity = baseCapacity;
        this.maxCredits = maxCredits;
        this.refillRate = refillRate;
    }

    public boolean allowRequest(String userId) {
        buckets.putIfAbsent(userId,
                new CreditBucket(baseCapacity, maxCredits, refillRate));
        return buckets.get(userId).allowRequest();
    }

    public static void main(String[] args) throws InterruptedException {
      UserCreditLimiter limiter = new UserCreditLimiter(
              3,   // base capacity
              5,   // max credits
              2    // refill 2 tokens/sec
      );
  
      // Burst at the beginning (full base capacity)
      System.out.println(limiter.allowRequest("u1")); // = true
      System.out.println(limiter.allowRequest("u1"));
      System.out.println(limiter.allowRequest("u1"));
      System.out.println(limiter.allowRequest("u1")); // likely false
  
      // Wait to accumulate credits
      Thread.sleep(3000); // earn 6 new tokens (2 tokens/sec)
  
      // Now burst again up to (3 + 5 = 8 requests)
      for (int i = 0; i < 8; i++) {
          System.out.println("Request " + i + ": " + limiter.allowRequest("u1"));
      }
  }

}


/*
ðŸŸ© 1. TOKEN BUCKET (Most common)
ðŸ’¡ INTUITION:

You have a bucket that fills with tokens at a fixed rate.

bucket_capacity = max allowed tokens
tokens refill steadily over time
each request consumes 1 token
if bucket empty â†’ reject

âœ” Allows BURST traffic

If bucket fills (say 10 tokens), user can send 10 requests instantly.

âœ” Smooth refill

Tokens come back at a fixed refill rate.

ðŸŽ¨ VISUAL
tokens added â†’ â†’ â†’ â†’ [ bucket ] â† requests remove tokens


Example:

capacity = 5
refill = 1 token per second


After 5 seconds of no traffic â†’ bucket is full â†’ you can burst 5 requests instantly.

ðŸ§  Summary (Simple)
Feature	Token Bucket
Burst allowed?	YES
Smooth requests?	YES
Reject if bucket empty?	YES
Refill tokens?	YES
ðŸŸ§ 2. LEAKY BUCKET (Different purpose)
ðŸ’¡ INTUITION:

Water (requests) enters the bucket.
Water leaks out at constant fixed rate.

If too much water comes at once â†’ bucket overflows â†’ reject request.

âœ” Smooths the OUTPUT

Even if requests come in bursts, output is smooth & regular.

âŒ NO burst allowed

Everything is forced to leak out slowly.

ðŸŽ¨ VISUAL
requests â†’ [ bucket ] â†’ (drip drip dripâ€¦) â†’ server


Water drips out at constant speed.

ðŸ§  Summary (Simple)
Feature	Leaky Bucket
Burst allowed?	NO
Output rate fixed?	YES
Reject if bucket overflows?	YES
Refill tokens?	NO (water leaks instead)
ðŸŸ¥ THE REAL DIFFERENCE (INTERVIEW-WINNING ANSWER)
â­ Token Bucket:

â€œEnforces average rate, but allows bursts.â€

â­ Leaky Bucket:

â€œEnforces constant rate, NO bursts.â€

This is EXACTLY what interviewers want.

ðŸŽ¯ EXAMPLE (Side-by-Side)
TOKEN BUCKET Example:
capacity = 5
refill = 1 token/sec

Requests:
- send 5 instantly â†’ ALLOWED
- sixth request â†’ REJECT

LEAKY BUCKET Example:
leak_rate = 1 request/sec

Requests:
- send 5 instantly â†’ only 1 goes through instantly
- the rest wait or get rejected (depending implementation)

ðŸŸ¦ Which one did we implement earlier?
Your rate limiter (capacity, leakRate):
tokens = tokens - leaked
tokens++


That is Leaky Bucket (constant leak).

Your credit system (allow burst up to maxCapacity + credits):

That becomes a Token Bucket.

ðŸŸ¨ Interview cheat sheet you say verbally:

If asked: "Explain Token Bucket"

â€œA bucket refills tokens at a constant rate.
Requests consume tokens.
Allows burst traffic up to bucket size.â€

If asked: "Explain Leaky Bucket"

â€œRequests go into bucket and leak out at constant rate.
Limits output rate. No burst allowed.â€

If asked: "Which is better?"

â€œFor rate limiting API hits â†’ token bucket (supports bursts).
For smoothing traffic to downstream system â†’ leaky bucket.â€

This is perfect Strong Hire answer.
*/

